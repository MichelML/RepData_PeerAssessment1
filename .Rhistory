library(randomForest)
library(rpart)
library(rpart.plot)
library(e1071)
set.seed(1210)
train_set <-read.csv("malearntrain.csv", na.strings=c("NA","#DIV/0!",
""))
test_set <- read.csv("machlearntest.csv", na.strings=c("NA","#DIV/0!", "
"))
dim(train_set)
dim(test_set)
train_set<-train_set[,colSums(is.na(train_set)) == 0]
test_set <-test_set[,colSums(is.na(test_set)) == 0]
train_set <-train_set[,‐c(1:7)]
test_set <-test_set[,‐c(1:7)]
dim(train_set)
dim(test_set)
head(train_set)
head(test_set)
subsamples <- createDataPartition(y=train_set$classe, p=0.75, list=FALSE)
subTraining <- train_set[subsamples, ]
subTesting <- train_set[‐subsamples, ]
dim(subTraining)
dim(subTesting)
head(subTraining)
head(subTesting)
plot(subTraining$classe, col="red", main="Bar Plot of levels of the variable classe within the subTr
aining data set", xlab="classe levels", ylab="Frequency")
mod1 <- rpart(classe ~ ., data=subTraining, method="class")
# Predicting:
pred1 <- predict(mod1, subTesting, type = "class")
# Plot of the Decision Tree
rpart.plot(mod1, main="Classification Tree", extra=102, under=TRUE, faclen=0)
mod2 <- randomForest(classe ~. , data=subTraining, method="class")
#packages:caret,randomForest,rpart,rpart.plot,e1071
library(caret)
library(randomForest)
library(rpart)
library(rpart.plot)
library(e1071)
set.seed(1210)
train_set <-read.csv("malearntrain.csv", na.strings=c("NA","#DIV/0!",
""))
test_set <- read.csv("machlearntest.csv", na.strings=c("NA","#DIV/0!", "
"))
dim(train_set)
dim(test_set)
train_set<-train_set[,colSums(is.na(train_set)) == 0]
test_set <-test_set[,colSums(is.na(test_set)) == 0]
train_set <-train_set[,‐c(1:7)]
test_set <-test_set[,‐c(1:7)]
dim(train_set)
dim(test_set)
head(train_set)
head(test_set)
subsamples <- createDataPartition(y=train_set$classe, p=0.75, list=FALSE)
subTraining <- train_set[subsamples, ]
subTesting <- train_set[‐subsamples, ]
dim(subTraining)
dim(subTesting)
head(subTraining)
head(subTesting)
plot(subTraining$classe, col="red", main="Bar Plot of classe levels", xlab="classe levels", ylab="Frequency")
mod1 <- rpart(classe ~ ., data=subTraining, method="class")
# Predicting:
pred1 <- predict(mod1, subTesting, type = "class")
# Plot of the Decision Tree
rpart.plot(mod1, main="classe Tree", extra=102, under=TRUE, faclen=0)
mod2 <- randomForest(classe ~. , data=subTraining, method="class")
mtcars$cyl<-as.factor(mtcars$cyl)
cyl<-mtcars$cyl
mpg<-mtcars$mpg
fit<-(mpg~cyl+(1*wt),data=mtcars)
fit<-(mpg~cyl+I(1*wt),data=mtcars)
fit<-lm(mpg~cyl+I(1*wt),data=mtcars)
fit
fit2<-lm(mpg~cyl,data=mtcars)
fit2$coef
fit3<-lm(mpg~(cyl*wt)+I(1*wt),data=mtcars)
fit3
summary(fit3)
summary(fit)
lm(fit~fit3)
lm(mpg ~ I(wt * 0.5) + factor(cyl), data = mtcars)
x <- c(0.586, 0.166, -0.042, -0.614, 11.72)
y <- c(0.549, -0.026, -0.127, -0.751, 1.344)
hat(x, intercept = TRUE)
mean(hat(x, intercept = TRUE))
mean(mean(hat(x, intercept = TRUE))+mean(hat(y, intercept = TRUE))
)
x <- c(0.586, 0.166, -0.042, -0.614, 11.72)
y <- c(0.549, -0.026, -0.127, -0.751, 1.344)
mean(hat(x, intercept = TRUE))
hat(x, intercept = TRUE)
help(dfbeta)
fit5\-lm(y~x)
fit5<-lm(y~x)
fit5<-lm(y~x)$coef
hat(fit5, intercept = TRUE)
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
colnames(segmentationOriginal)
segmentationOriginal("Case")
segmentationOriginal["Case"]
test<-segmentationOriginal[(segmentationOriginal$Case == "Test"),]
test
train<-segmentationOriginal[(segmentationOriginal$Case == "Train"),]
train
rpart1 <- rpart(Class ~ ., data = train, control = rpart.control(maxdepth = 2))
library(rpart)
rpart1 <- rpart(Class ~ ., data = train, control = rpart.control(maxdepth = 2))
rpart1
set.seed(125)
rpart1 <- rpart(Class ~ ., data = train, control = rpart.control(maxdepth = 2))
rpart1
rpart2 <- rpart(Class ~ ., data = test, control = rpart.control(maxdepth = 2))
rpart2
plot.rpart(rpart2)
library(plot.rpart)
install.packages(partykit)
install.packages("partykit")
library(partykit)
rpart2a<-as.party(rpart2)
plot(rpart2a)
rpartFull <- rpart(Class ~ ., data = test)
rpartFulla<-as.party(rpartFull)
plot(rpartFull)
plot(rpartFull)
library(rpart.plot)
plot(rpartFulla)
plot(rpart1)
plot(rpart2a)
plot(rpart1a)
rpart1<-as.party(rplot1)
rpart1<-as.party(rpart1)
plot(rpart1)
colnames(train)
rpartFulltrain<-rpart(Class~.,data=train)
rpartfulltr<-as.party(rpartFulltrain)
plot(rpartfulltr)
rpartPred <- predict(rpartFull, testing, type = "class")confusionMatrix(rpartPred, testing$Class) # requires 2 factor vectors
rpartPred <- predict(rpartFull, test, type = "class")
confusionMatrix(rpartPred, test$Class) # requires 2 factor vectors
rpartPreda<-as.party(rpartPred)
rpartFulltest<-rpart(Class~.,data=test)
rpartfulltr<-as.party(rpartFulltest)
plot(rpartfulltr)
rpart.plot(rpartPred)
summary(rpartPred)
rpartFulltrain<-rpart(Class~.,data=train)
rpartfulltr<-as.party(rpartFulltrain)
plot(rpartfulltr)
set.seed(125)
test<-segmentationOriginal[(segmentationOriginal$Case == "Test"),]
train<-segmentationOriginal[(segmentationOriginal$Case == "Train"),]
rpartFull <- rpart(Class ~ ., data = train)
rpartFulla<-as.party(rpartFull)
plot(rpartFull)
plot(rpartFulla)
plot.rpart(rpartFull)
library(rpart.plot)
plot(rpartFull)
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
library(rpart);library(rpart.plot)
set.seed(125)
test<-segmentationOriginal[(segmentationOriginal$Case == "Test"),]
train<-segmentationOriginal[(segmentationOriginal$Case == "Train"),]
rpartFull <- rpart(Class ~ ., data = train)
plot(rpartFull)
plot(rpartFull, uniform = FALSE, branch = 1, compress = FALSE, nspace,
margin = 0, minbranch = 0.3, ...)
plot(rpartFull, uniform = FALSE, branch = 1, compress = FALSE, nspace,
margin = 0, minbranch = 0.3)
plot(rpartFull)
rpart.plot(rpartFull, main="classe Tree", extra=102, under=TRUE, faclen=0)
rpartFull <- rpart(Class ~ ., data = train, method="class")
rpart.plot(rpartFull, main="classe Tree", extra=102, under=TRUE, faclen=0)
rpartFull
rpartFull <- rpart(Class ~ ., data = train, method="class", maxDepth = 100)
rpart1 <- rpart(Class ~ ., data = train, control = rpart.control(maxdepth = 100))
rpart1 <- rpart(Class ~ ., data = train, control = rpart.control(maxdepth = 30))
rpart.plot(rpart1, main="classe Tree", extra=102, under=TRUE, faclen=0)
par(mfrow=c(1,1))
plot(0:200,dpois(0:200,lambda=100),type="h",frame=FALSE)
?shuttle
library(MASS)
colnames(shuttle)
shuttle$use
logauto<-glm(shuttle$use~shuttle$wind,family="binomial")
logauto
summary(logauto)
anova(logauto,test="Chisq")
logauto<-glm(shuttle$use~shuttle$wind-1,family="binomial")
summary(logauto)
logauto<-glm(shuttle$use~shuttle$wind,family="binomial")
summary(logauto)
logauto<-glm(shuttle$use~shuttle$wind+shuttle$magn,family="binomial")
summary(logauto)
InsectSprays
insect<-InsectSprays[(InsectSprays$sapray =="A" |InsectSprays$sapray =="B"),]
insect
insect<-InsectSprays[(InsectSprays$spray =="A" |InsectSprays$spray =="B"),]
insect
insect1<-InsectSprays
insect1$spray<-as.factor(insect1$spray)
glm1<-glm(insect$count~insect$spray,family="poisson")
summary(glm1)
glm1<-glm(insect$count~insect$spray,family="poisson")
plot(insect$count,insect$spray,pch=19,col="blue")
lines(insect$count,glm1$fitted,col="red",lwd=3)
abline(lm1,col="red",lwd=3);lines(insect$count,glm1$fitted,col="red",lwd=3)
abline(glm1,col="red",lwd=3);lines(insect$count,glm1$fitted,col="red",lwd=3)
library(caret)
library(kernlab)
data(spam)
inTrain<-createDataPartition(y=spam$type,p=0,75,listFALSE)
inTrain<-createDataPartition(y=spam$type,p=0,75,list=FALSE)
inTrain<-createDataPartition(y=spam$type,p=0.75,list=FALSE)
training<-spam[inTrain,]
testing<-[-inTrain,]
testing<-spam[-inTrain,]
training
testing
dim(training)
dim(testing)
set.seed(333)
modelFit<-train(type~.,data=training,method="glm")
modelFit
modelFit$finalModel
predictions<-predict(modelFit,newdata=testing)
predictions
confusinMatri(predictions,testing$type)
confusionMatrix(predictions,testing$type)
folds<-createFolds(y=spam$type,k=10,list=TRUE,returnTrain=FALSE)
sapply(folds,lenght)
sapply(folds,length)
folds<-createFolds(y=spam$type,k=10,list=TRUE,returnTrain=TRUE)
sapply(folds,length)
folds<-createResample(y=spam$type,times=10,list=TRUE)
folds
folds<-createTimeSlices(y=tme,initialWindow=20,horizon=20)
tme<-1:1000
folds<-createTimeSlices(y=tme,initialWindow=20,horizon=20)
folds
folds$train[[1]]
folds$test[[1]]
args(colnames.default)
args(nrow.default)
args(createTimesSlices)
args(train.daulft)
args(train.default)
library(ISLR);library(ggplot2);library(caret)
data(Wage)
summary(Wage)
inTrain<-createDataPartition(y=Wage$wage,p=0.7,list=FALSE)
training<-Wage[inTrain,]
testing<-Wage[-inTrain,]
dim(training)
dim(testing)
featurePlot(x=training[,c("age","education","jobclass")],y=training$Wage.plot="pairs")
featurePlot(x=training[,c("age","education","jobclass")],y=training$Wage,plot="pairs")
dim(testing)
featurePlot(x=training[,c("age","education","jobclass")],y=training$wage.plot="pairs")
featurePlot(x=training[,c("age","education","jobclass")],y=training$wage,plot="pairs")
library(ISLR);library(ggplot2);library(caret)
data(Wage)
summary(Wage)
inTrain<-createDataPartition(y=Wage$wage,p=0.7,list=FALSE)
training<-Wage[inTrain,]
testing<-Wage[-inTrain,]
dim(training)
dim(testing)
featurePlot(x=training[,c("age","education","jobclass")],y=training$wage,plot="pairs")
install.packages("ISLR")
library(ISLR);library(ggplot2);library(caret)
data(Wage)
summary(Wage)
inTrain<-createDataPartition(y=Wage$wage,p=0.7,list=FALSE)
training<-Wage[inTrain,]
testing<-Wage[-inTrain,]
dim(training)
dim(testing)
featurePlot(x=training[,c("age","education","jobclass")],y=training$wage,plot="pairs")
qplot(age,wage,data=training)
qplot(age,wage,colour=jobclass,data=training)
qq<-qplot(age,wage,colour=education,data=training)
qq+geom_smooth(method="lm",formula=y~x)
cutWage<-cut2(training$wage,g=3)
table(cutWage)
??cut2
library(Hmisc)
library(Hmisc)
cutWage<-cut2(training$wage,g=3)
cutWage
table(cutWage)
p1<-qplot(cutWage,age,data=training,fill=cutWage,geom=c("boxplot"))
p1
p1<-qplot(cutWage,age,data=training,fill=cutWage,geom=c("boxplot",jitter));grid.arrange(p1,p2,ncol=2)
p1<-qplot(cutWage,age,data=training,fill=cutWage,geom=c("boxplot","jitter"));grid.arrange(p1,p2,ncol=2)
??grid.arrange
??grid
library(grid)
p1<-qplot(cutWage,age,data=training,fill=cutWage,geom=c("boxplot","jitter"));grid.arrange(p1,p2,ncol=2)
library(gridBase)
install.packages("gridBase")
p2<-qplot(cutWage,age,data=training,fill=cutWage,geom=c("boxplot","jitter"));grid.arrange(p1,p2,ncol=2)
p1<-qplot(cutWage,age,data=training,fill=cutWage,geom=c("boxplot"))
p2<-qplot(cutWage,age,data=training,fill=cutWage,geom=c("boxplot","jitter"))
grid.arrange(p1,p2,ncol=2)
install.packages("gridExtra")
p1<-qplot(cutWage,age,data=training,fill=cutWage,geom=c("boxplot"))
p2<-qplot(cutWage,age,data=training,fill=cutWage,geom=c("boxplot","jitter"))
grid.arrange(p1,p2,ncol=2)
library(gridExtra)
p1<-qplot(cutWage,age,data=training,fill=cutWage,geom=c("boxplot"))
p2<-qplot(cutWage,age,data=training,fill=cutWage,geom=c("boxplot","jitter"))
grid.arrange(p1,p2,ncol=2)
qplot(wage,colour=education,data=training,geom="density")
inTrain<-createDataPartition(y=spam$type,p=0.75,list=FALSE)
training<-spam[inTrain,]
testing<-spam[-inTrain,]
dim(training)
dim(testing)
hist(training$capitalAve,main="",xlab="ave capital run length")
mean(training$capitalAve)
sd(training$capitalAVe)
sd(training$capitalAve)
trainCap<-training$capitalAve
trainCap_standard<-(trainCap- mean(trainCap))/sd(trainCapAve)
trainCap_standard<-(trainCap- mean(trainCap))/sd(trainCap)
mean(trainCap_standard)
sd(trainCap_standard)
trainCap<-training$capitalAve
trainCap_standard<-(trainCap- mean(trainCap))/sd(trainCap)
testCap<-testing$capitalAve
testCap_standard<-(testCap- mean(testCap))/sd(testCap)
#tou can use preProcess also
preObj<-preProcess(training[,-58],method=c("center","scale"))
trainCap<-predict(preObj,training[,-58])$capitalAve
testCap<-predict(preObj,testing[,-58])$capitalAve
##Box-Cox transforms can also be used to standardize
##imputing data (removing missing values)
training$capAve<-training$capitalAve
selectNA<-rbinom(dim(training)[1],size=1,prob=0.05)==1
training$capAve[selectNA]<-NA
#impute and standardize
preObj<-preProcess(training[,-58],method="knnImpute")
capAve<-predict(preObj,training[,-58])$capAve
capAvetruth<-training$capitalAve
capAvetruth<-(capAvetruth-mean(capAvetruth))/sd(capAvetruth)
library(RANN)
install.packages(RANN)
install.packages("RANN")
library(RANN)
trainCap<-training$capitalAve
trainCap_standard<-(trainCap- mean(trainCap))/sd(trainCap)
testCap<-testing$capitalAve
testCap_standard<-(testCap- mean(testCap))/sd(testCap)
#tou can use preProcess also
preObj<-preProcess(training[,-58],method=c("center","scale"))
trainCap<-predict(preObj,training[,-58])$capitalAve
testCap<-predict(preObj,testing[,-58])$capitalAve
##Box-Cox transforms can also be used to standardize
##imputing data (removing missing values)
training$capAve<-training$capitalAve
selectNA<-rbinom(dim(training)[1],size=1,prob=0.05)==1
training$capAve[selectNA]<-NA
#impute and standardize
preObj<-preProcess(training[,-58],method="knnImpute")
capAve<-predict(preObj,training[,-58])$capAve
capAvetruth<-training$capitalAve
capAvetruth<-(capAvetruth-mean(capAvetruth))/sd(capAvetruth)
dummmies<-dummyVars(wage~jobclass,data=training)
iris
modFit<-train(Species~.,method="rpart",data=training)
data(iris)
modFit<-train(Species~.,method="rpart",data=training)
modFit<-train(Species~.,method="rpart",data=iris)
print(modFit$finalModel)
plot(modFit$finalModel,uniform=TRUE)
plot(modFit$finalModel,uniform=TRUE)
text(modFit$finalModel,use.n=TRUE,all=TRUE,cex=.8)
install.packages("rattle")
library(rattle)
fancyRpartPlot(modFit$finalModel)
install.packages("ElemStatLearn")
data(ozone,package="ElemStatLearn")
ozone<-ozone[order(ozone$ozone),]
head(ozone)
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
seg<-segmentationOriginal
training<-seg[(seg$Case =="train"),]
training
seg$Case
training<-seg[(seg$Case =="Train"),]
testing<-seg[(seg$Case=="Test"),]
set.seed(125)
modFit<-train(Case~.,method="rpart",data=training)
set.seed(125)
training<-seg[(seg$Case =="Train"),]
testing<-seg[(seg$Case=="Test"),]
modFit<-train(Case~.,method="rpart",data=training)
dim(training)
dim(testing)
modelFit<-train(Case~.,data=training,method="glm")
fancyRpartPlot(modelFit$finalModel)
modelFit<-train(Case~.,data=training,method="rpart")
inTrain<-createDataPartition(y=seg$Case,p=0.75,list=FALSE)
training<-seg[inTrain,]
testing<-seg[-inTrain,]
dim(training)
dim(testing)
modFit<-train(Case~.,method="rpart",data=seg)
fancyRpartPlot(modFit$finalModel)
help(train)
inTrain <- createDataPartition(y = segmentationOriginal$Case, list = FALSE)
train <- subset(segmentationOriginal, Case == "Train")
test <- subset(segmentationOriginal, Case == "Test")
# 2. Set the seed to 125 and fit a CART model with the rpart method using all
# predictor variables and default caret settings.
modFit <- train(Class ~ ., data = train, method = "rpart")
modFit$finalModel
# 3. In the final model what would be the final model prediction for cases with the following variable values:
# Look at the output
# a. TotalIntench2 = 23,000; FiberWidthCh1 = 10; PerimStatusCh1=2       PS
# b. TotalIntench2 = 50,000; FiberWidthCh1 = 10;VarIntenCh4 = 100       WS
# c. TotalIntench2 = 57,000; FiberWidthCh1 = 8;VarIntenCh4 = 100        PS
# d. FiberWidthCh1 = 8;VarIntenCh4 = 100; PerimStatusCh1=2              Not possible to predict
plot(modFit$finalModel, uniform = TRUE, main = "Classification Tree")
text(modFit$finalModel, use.n = TRUE, all = TRUE, cex = .8)
fancyRpartPlot(modFit$finalModel)
fancyRpartPlot(modFit)
predict(modFit, newdata = train)
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
library(rpart)
install.packages(rattle)
set.seed(125)
# 1. Subset the data to a training set and testing set based on the Case variable in the data set.
inTrain <- createDataPartition(y = segmentationOriginal$Case, list = FALSE)
train <- subset(segmentationOriginal, Case == "Train")
test <- subset(segmentationOriginal, Case == "Test")
# 2. Set the seed to 125 and fit a CART model with the rpart method using all
# predictor variables and default caret settings.
modFit <- train(Class ~ ., data = train, method = "rpart")
modFit$finalModel
# 3. In the final model what would be the final model prediction for cases with the following variable values:
# Look at the output
# a. TotalIntench2 = 23,000; FiberWidthCh1 = 10; PerimStatusCh1=2       PS
# b. TotalIntench2 = 50,000; FiberWidthCh1 = 10;VarIntenCh4 = 100       WS
# c. TotalIntench2 = 57,000; FiberWidthCh1 = 8;VarIntenCh4 = 100        PS
# d. FiberWidthCh1 = 8;VarIntenCh4 = 100; PerimStatusCh1=2              Not possible to predict
plot(modFit$finalModel, uniform = TRUE, main = "Classification Tree")
text(modFit$finalModel, use.n = TRUE, all = TRUE, cex = .8)
fancyRpartPlot(modFit$finalModel)
fancyRpartPlot(modFit)
predict(modFit, newdata = train)
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
seg<-segmentationOriginal
inTrain<-createDataPartition(y=seg$Case,p=0.75,list=FALSE)
training<-seg[inTrain,]
testing<-seg[-inTrain,]
modFit<-train(Case~.,method="rpart",data=seg)
fancyRpartPlot(modFit$finalModel)
set.seed(125)
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
seg<-segmentationOriginal
inTrain<-createDataPartition(y=seg$Case,p=0.75,list=FALSE)
training<-seg[inTrain,]
testing<-seg[-inTrain,]
modFit<-train(Case~.,method="rpart",data=seg)
fancyRpartPlot(modFit$finalModel)
modFit<-train(Class~.,method="rpart",data=seg)
fancyRpartPlot(modFit$finalModel)
set.seed(125)
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
seg<-segmentationOriginal
training<-seg[(inTrain)seg$Case=="Train"),]
training<-seg[(seg$Case=="Train"),]
testing<-seg[(seg$Case=="Test"),]
modFit<-train(Class~.,method="rpart",data=seg)
fancyRpartPlot(modFit$finalModel)
fancyRpartPlot(modFit$finalModel)
modFit
modFit$finalModel
predictions<-predict(modFit,newdata=testing)
predictions
fancyRpartPlot(predictions)
confusinMatrix(predictions,testing$Class)
confusionMatrix(predictions,testing$Class)
library(ElemStatLearn)
data(SAheart)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[‐train,]
set.seed(13234)
colnames(concrete)
data(concrete)
colnames(concrete)
